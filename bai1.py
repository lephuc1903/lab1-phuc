# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15lIq-_OXL0E1nNg9TtR3ya01_mW26hG6
"""

# !pip install pyspark
# !ls drive/MyDrive/bigdata/bai1

# from google.colab import drive
# drive.mount('/content/drive')

from pyspark import SparkConf, SparkContext
import collections

file = open("text.txt")
data = (''.join(e for e in file.read() if e.isalnum() or e == ' ')).lower().split(' ')

words_counted = SparkContext.getOrCreate(conf=SparkConf().setMaster("local").setAppName("word counting")).parallelize(data).map(lambda word: (word, 1)).reduceByKey(lambda x,y: x+y).collect()
words_counted.sort(key=lambda x: x[1], reverse=True)

print("There are total of "+str(sum([i[1] for i in words_counted]))+" words")
print("There are total of "+str(len(words_counted))+" type of words")
print("Frequency of words appear in text file in deceasing order: (more than 10)")
_ = [print(words_counted[i][0]+":",words_counted[i][1]) for i in range(0, len(words_counted)) if (words_counted[i][1] > 10)]